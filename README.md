# HuGPT-Abstracts-detector
An approach for detecting abstracts written by ChatGPT in the field of computer science
In the field of computer science, there has been no comprehensive, up-to-date, and labeled dataset for detecting abstracts generated by ChatGPT. Therefore, in this project, we constructed a balanced and comprehensive dataset consisting of:

1040 human-written abstracts

1040 corresponding ChatGPT-generated abstracts

This dataset was created and utilized in the study. Full details of the dataset are available at this [link](link to your GitHub page).

Models Used

Four main models were employed in this project:

BERT

RoBERTa

BERT with 9 selected linguistic and statistical features

RoBERTa with 9 selected linguistic and statistical features

Selected Linguistic and Statistical Features

Average sentence length: Measures the mean number of words per sentence, serving as an indicator of syntactic complexity.

Average word length: Represents the mean number of characters per word, reflecting the degree of formality or technicality of the vocabulary.

Ratio of unique words: Calculates the proportion of non-repetitive words to all words, capturing lexical diversity and richness.

Number of grammatical errors: Evaluates the frequency of syntactic or grammatical mistakes, indicating textual accuracy and cohesion.

Number of discourse markers: Examines words and phrases such as however, therefore, and in addition, which contribute to logical flow and textual coherence.

Ratio of nouns, pronouns, verbs, adverbs, and adjectives: Analyzes part-of-speech distribution to capture stylistic and structural properties.

Ratio of punctuation marks to total words: Measures the relative frequency of punctuation usage, reflecting rhythm and textual organization.

Ratio of stop words: Indicates the proportion of frequent functional words (e.g., the, and, is) that influence stylistic uniformity or variation.

Ratio of emotional words: Captures the presence of affective or subjective terms such as important, critical, and unfortunate, which are generally more varied in human writing.

Evaluation Process

The models were trained and evaluated using 5-fold cross-validation.

The entire training and evaluation procedure was repeated 10 times.

The average of the obtained results was then reported.

Results

Our primary evaluation metric was Recall.
The rationale behind choosing Recall is that, in this task, it is highly important to correctly identify all ChatGPT-generated abstracts. In other words, false negatives—cases where a ChatGPT-generated abstract is misclassified as human-written—pose a more critical issue than false positives.

The results demonstrated that our approach achieved a 99.9% Recall in detecting ChatGPT-generated abstracts in the field of computer science.
